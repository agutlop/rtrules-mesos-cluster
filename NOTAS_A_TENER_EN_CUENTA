Creacion del cluster:
    - vagrant up
    - cuando este el nodo1 creado, lanzar mesos dns
    - cambiar spark-env de cada nodo para poner el local IP
    - una vez creado el cluster levantar el spark dispatcher


---- PENDIENTE:
modificar el ansible playbook para lanzar en el arranque:
    - mesos dns (ojo, esto era la razon de q no se instalara nada en los siguientes nodos...)
    - Spark dispatcher (en el arranque o al final del todo....)
    - instalar kafka en el master

crear una nueva configuracion
copiar configuracion del nodo1 sobre spar-default y spark-env a todos

--------- FIN PENDIENTE ----------------------------------------------------------------------------------

---- KAFKA INSTALACION
git clone https://github.com/mesos/kafka
cd kafka
./gradlew jar
wget https://archive.apache.org/dist/kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz

Lanzar el scheduler(intentar hacerlo desde marathon)
1. Modificar el fichero 
    /home/vagrant/mesosKafka/kafka-mesos.properties
    
    [vagrant@node1 mesosKafka]$ cat kafka-mesos.properties
    # Scheduler options defaults. See `./kafka-mesos.sh help scheduler` for more details
    debug=true
    user=vagrant
    storage=zk:/mesos-kafka-scheduler
    master=node1:5050
    zk=node1:2181/mesos

    #for testing on the vagrant master via ./kafka-mesos.sh scheduler
    #you will eventually want to run this on a scheduler i.e marathon
    #change the IP to what is service discoverable & routable for your setup
    api=http://192.168.33.10:7000
    
2. lanzar
    ./kafka-mesos.sh scheduler

3. a単adir brokers (ver opciones)
    ./kafka-mesos.sh broker add 0 --cpus 0.5 --heap 300 --mem 300

4. a単adir topics (ver opciones)
    ./kafka-mesos.sh topic add topic-wordapp --broker 0
    ./kafka-mesos.sh topic add topic-wordcountapp --broker 0
    ./kafka-mesos.sh topic add topic-meetuprsvp --broker 0
    
5. Para poder lanzar desde marathon se a単ade la siguiente linea al principio del script kafka-mesos.sh

cd /home/vagrant/mesosKafka

Marathon Config:

{
  "id": "/kafka-scheduler",
  "cmd": "sh /home/vagrant/mesosKafka/kafka-mesos.sh scheduler",
  "cpus": 0.1,
  "mem": 32,
  "disk": 0,
  "instances": 1,
  "constraints": [
    [
      "hostname",
      "CLUSTER",
      "node1"
    ]
  ],
  "portDefinitions": [
    {
      "port": 10001,
      "protocol": "tcp",
      "labels": {}
    }
  ]
}



Escritura / lectura desde windows!!!

D:\TFM\kafka_2.10-0.8.2.2\kafka_2.10-0.8.2.2\bin\windows>kafka-console-consumer.bat --zookeeper node1:2181/mesos --topic topic-prueba --from-beginning
D:\TFM\kafka_2.10-0.8.2.2\kafka_2.10-0.8.2.2\bin\windows>kafka-console-producer.bat --broker-list node2:31000 --topic topic-wordapp


----- FIN KAFKA -------------------------------------------------------------------------------

--- MESOS DNS -
*ver ejercicio 17 de mesosphere...

Esta seria mesosdns :

{
  "id": "/mesos-dns",
  "cmd": "sudo /home/vagrant/go/src/github.com/mesosphere/mesos-dns/mesos-dns -v=1 -config=/home/vagrant/go/src/github.com/mesosphere/mesos-dns/config.json",
  "cpus": 0.5,
  "mem": 32,
  "disk": 0,
  "instances": 1,
  "constraints": [
    [
      "hostname",
      "CLUSTER",
      "node1"
    ]
  ],
  "portDefinitions": [
    {
      "port": 10000,
      "protocol": "tcp",
      "labels": {}
    }
  ]
}
--comando post
curl -X POST http://192.168.33.10:8080/v2/apps -d @/vagrant/marathon-apps/mesos-dns.json -H "Content-type: application/json"

-----------FIN MESOS DNS ------------------------------------------------------------------------------------------------------


--- SPARK

(se supone que esto no se puede lanzar asi desde marathon)
lanzar start-mesos-dispatcher (comprobar como hacerlo por marathon y como a単adir config de zookeeper)
    ./start-mesos-dispatcher.sh -m mesos://192.168.33.10:5050


submitir el job...

spark-submit --class Prueba spark-pruebas-assembly-1.0.jar --master mesos://192.168.33.10:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 350M \
  --total-executor-cores 7 
  
  spark-submit --class Prueba spark-pruebas-assembly-1.0.jar --master mesos://192.168.33.10:5050 --deploy-mode cluster --executor-memory 350M --total-executor-cores 7

spark-submit --class Prueba spark-pruebas-assembly-1.0.jar 




Poner kafka como framework con un worker... si es posible, si no, como un servicio aparte...


--CONFIGURACION en spark-defaults.conf

#pasar por parametro
spark.app.name               "Pruebas"
spark.driver.cores           1
spark.driver.maxResultSize   2g
spark.driver.memory          600m
spark.executor.memory        600m
spark.master                 mesos://192.168.33.10:7077
spark.submit.deployMode      cluster
spark.executor.cores         0.8
spark.cores.max              7



spark.executor.memory
spark.executor.cores
spark.cores.max
spark.mesos.coarse true

#
spark.mesos.executor.home $SPARK_HOME


{
    "id": "spark-dispatcher",
    "cmd": "/opt/spark/current/bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher --master \"mesos://192.168.33.10:5050\"",
    "mem": 512,
    "cpus": 0.5,
    "instances": 1,
    "disk": 0.0,
    "ports": [0]
}





tirar servicios

sudo service zookeeper-server stop
sudo service mesos-master stop
sudo service mesos-slave stop
sudo service marathon stop
sudo service chronos stop


levantar servicios
sudo service zookeeper-server start
sudo service mesos-master start
sudo service mesos-slave start
sudo service marathon start
sudo service chronos start




spark-submit --class org.ant.examples.ESWordsApp --master mesos://192.168.33.10:7077 --deploy-mode cluster --driver-cores 0.5 --executor-memory 600M --total-executor-cores 7 spark-pruebas-assembly-1.0.jar node2:31000 192.168.33.1 27017 rtrules_db

spark-submit --class org.ant.examples.MeetupRSVPAppES --master mesos://192.168.33.10:7077 --deploy-mode cluster --driver-cores 0.5 --executor-memory 600M --total-executor-cores 7 spark-pruebas-assembly-1.0.jar node2:31000 192.168.33.1 27017 rtrules_db



spark-submit --class org.ant.examples.ESWordsApp --master local[2] spark-pruebas-assembly-1.0.jar node2:31000 192.168.33.1 27017 rtrules_db


./bin/spark-submit \
  --class org.ant.examples.ESWordsApp \
  --master mesos://192.168.33.10:7077 \
  --deploy-mode cluster \
  --driver-memory 600M \
  --executor-memory 600M \
  --total-executor-cores 6 \
  --jars local:/elasticsearch-spark-20_2.11-5.0.0-alpha5.jar \
  spark-pruebas-assembly-1.0.jar \
  node1:31000 192.168.33.1 27017 rtrules_db

spark-submit --kill driver-20160922125426-0003 --master mesos://192.168.33.10:7077

spark.driver.cores           1
spark.driver.maxResultSize   2g
spark.driver.memory          600m
spark.executor.memory        600m
spark.master                 mesos://192.168.33.10:7077
spark.submit.deployMode      cluster
spark.executor.cores         0.8
spark.cores.max              7


----------- ELASTIC SEARCH -----------------

network.bind_host: ["192.168.33.1", "localhost"]
network.publish_host: "localhost"


elasticsearch.bat

Posibles configuraciones:



curl -XDELETE 'http://localhost:9200/twitter/'

curl -XGET 'http://localhost:9200/topic-wordapp/'


crear un indice
curl -DELETE 'http://localhost:9200/topic-wordapp'



--CREAR INDICE
curl -XPUT 'http://localhost:9200/topic-wordapp/' -d '{"settings" : {"index" : {"number_of_shards" : 5, "number_of_replicas" : 1 } }}'
curl -XPUT 'http://localhost:9200/topic-wordcountapp/' -d '{"settings" : {"index" : {"number_of_shards" : 5, "number_of_replicas" : 1 } }}'
curl -XPUT 'http://localhost:9200/topic-meetuprsvp/' -d '{"settings" : {"index" : {"number_of_shards" : 5, "number_of_replicas" : 1 } }}'


--PONER MAPPING
--rule name, string not_analyzed
curl -XPUT 'http://localhost:9200/topic-wordapp/_mapping/{rule_name}' -d '{"properties": { "rule_name": { "type": "string", "index": "not_analyzed" }}}'
curl -XPUT 'http://localhost:9200/topic-wordapp/_mapping/{rule_name}' -d '{"properties": { "time": { "type": "date" }}}'
curl -XPUT 'http://localhost:9200/topic-wordapp/_mapping/{rule_name}' -d '{"properties": { "result": {"type": "string"}}}'

curl -XPUT 'http://localhost:9200/topic-wordcountapp/_mapping/{rule_name}' -d '{"properties": { "rule_name": { "type": "string", "index": "not_analyzed" }}}'
curl -XPUT 'http://localhost:9200/topic-wordcountapp/_mapping/{rule_name}' -d '{"properties": { "time": { "type": "date" }}}'
curl -XPUT 'http://localhost:9200/topic-wordcountapp/_mapping/{rule_name}' -d '{"properties": { "result": {"type": "string"}}}'


curl -XPUT 'http://localhost:9200/topic-meetuprsvp/_mapping/{rule_name}' -d '{"properties": { "rule_name": { "type": "string", "index": "not_analyzed" }}}'
curl -XPUT 'http://localhost:9200/topic-meetuprsvp/_mapping/{rule_name}' -d '{"properties": { "time": { "type": "date" }}}'
curl -XPUT 'http://localhost:9200/topic-meetuprsvp/_mapping/{rule_name}' -d '{"properties": { "result": {"type": "string"}}}'


